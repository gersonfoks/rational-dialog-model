{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddd62aff",
   "metadata": {},
   "source": [
    "In this notebook we look at the language model class and the lightning wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37a2595f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we fix the relative imports\n",
    "import os\n",
    "import sys\n",
    "# M\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1c82fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.LanguageModels.LstmLanguageModel import LSTMLanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb5673da",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Every language model should implement the interface defined in the base language model\n",
    "\n",
    "#We first look at a small example lstm based language model\n",
    "\n",
    "from daily_dialog.DialogTokenizer import get_daily_dialog_tokenizer\n",
    "my_tokenizer = get_daily_dialog_tokenizer(tokenizer_location='../daily_dialog/tokenizer.json', )\n",
    "\n",
    "\n",
    "#This imports a fresh language model\n",
    "language_model = LSTMLanguageModel(my_tokenizer.get_vocab_size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "847ba7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([427, 219, 178, 127, 484,  33,   1])\n",
      "tensor([ 427,  219,  178,  127,  484,   33,    1,  549,  817,  430,  817,  150,\n",
      "        1020, 1402,  817,  994, 1183, 2028, 1698,  817,  636,  150, 1020, 1468,\n",
      "        1053,  673,  994,  430, 2028,  994, 2044, 1323, 1402,  929,  549,  102,\n",
      "        1165,  121,  549,  549, 1610,  712, 1610, 1402,  517,  712,  517, 1053,\n",
      "         871,  202,  817, 1402,  929,  430,  549,  509, 1183,  666,  827, 1183,\n",
      "        2028, 1402, 1721, 1020,  430,  549,  150,  509,  666,  549,  871,  871,\n",
      "        1364, 1183, 1721,  858, 1835, 1835,   79, 1183, 1835, 1183, 1020, 1020,\n",
      "          79,   79,  693, 1183,  549, 1721,  430, 1721,  929, 1835, 1183, 1835,\n",
      "         191,  858,  673,  994])\n"
     ]
    }
   ],
   "source": [
    "#We can give a context and ask to complete it\n",
    "from utils.utils import encode\n",
    "context = \"Hi how are you today? [SEP]\"\n",
    "ids = encode(my_tokenizer, context)\n",
    "print(ids)\n",
    "completed_dialogue = language_model.complete_dialogue(ids)\n",
    "print(completed_dialogue)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47e28dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi how are you today ? [SEP] ke price lot priceed prefer beijing price seemswh quiet fact pricetinged prefercially probably el seems lot quiet seems div wonderful beijing understand ke4 weather se ke ke popular around popular beijingty aroundty probably enoughim price beijing understand lot kebewhqu pretwh quiet beijing system prefer lot keedbequ ke enough enoughvedwh systemember soft softcwh softwh prefer preferccousewh ke system lot system understand softwh softionember el seems\n"
     ]
    }
   ],
   "source": [
    "## we decode the completed dialogue to see its response, which mostly nonsense as it is not trained yet\n",
    "from utils.utils import decode\n",
    "decoded_completed_dialogue = decode(my_tokenizer, completed_dialogue)\n",
    "print(decoded_completed_dialogue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2da8daf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loading a trained model, which you can see already gives a better result, it is locally consistent.\n",
    "location = \"../models/small_lm_test.pt\"\n",
    "trained_language_model = LSTMLanguageModel.load(location) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fb7c4b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([427, 219, 178, 127, 484,  33,   1])\n",
      "tensor([ 427,  219,  178,  127,  484,   33,    1,  427,   18,   51, 1461,   16,\n",
      "         176,  177,   47,  159,  163,  127,   33,    1,   47,   11,   51, 1290,\n",
      "         130,  557,  127,  269, 1324,  130,  303,   18,    1,   47,  171,  269,\n",
      "         699, 1311,  163,  127,   18,   47,   73,  302,  830,  128,   54,  959,\n",
      "          81,  130,  175,   18,    1,  284, 1748,  499,  127,  553,   39,  241,\n",
      "         565,   85,  634,  661,   33,    1,  157,   11,   57,  281,   18,    1,\n",
      "         219,  234,  633,  535,   33,    1,  479,   16,  140,   11,   57,  128,\n",
      "         451,  446,   18,    1,  284,  127,  254,  423,   47,   11,   42, 1298,\n",
      "         162,  212,  127,   33])\n",
      "hi how are you today ? [SEP] hi . mike , what can i do for you ? [SEP] i ' m calling to ask you somewhere to work . [SEP] i have some bad news for you . i ’ ve heard the piano to me . [SEP] did anyone say you were a good mangary class ? [SEP] that ' s right . [SEP] how about tomorrow morning ? [SEP] yeah , it ' s the first thing . [SEP] did you know where i ' d rather go with you ?\n"
     ]
    }
   ],
   "source": [
    "ids = encode(my_tokenizer, context)\n",
    "print(ids)\n",
    "completed_dialogue = trained_language_model.complete_dialogue(ids)\n",
    "print(completed_dialogue)\n",
    "decoded_completed_dialogue = decode(my_tokenizer, completed_dialogue)\n",
    "print(decoded_completed_dialogue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df17d73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### We wrape the language model into a pytorch lightning module. This makes training convinient and adds some more high-level functions.\n",
    "# But it also requires giving some extra information to the model such as the hyperparameters\n",
    "from modules.pytorch_lightning.LightningLanguageModel import LightningLanguageModel\n",
    "hparams = {\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"teacher_forcing\": True,\n",
    "    \"freeze_language_ml\": True\n",
    "}\n",
    "\n",
    "lightning_language_model = LightningLanguageModel(trained_language_model, my_tokenizer, hparams=hparams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df1edc95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"hi how are you ? [SEP] hello , may i have a look at this time of bed or canada ? [SEP] no , thank you very much . [SEP] i ' m afraid i could . do you have any other questions ? [SEP] yes , i ' m in , too , i haven ’ t found anything in our new computer and i have worked in the same department . [SEP] i ' m not really sure to see you ? [SEP] yes , we will . [SEP] what about your family ? [SEP] do\", \"what are you upto ? [SEP] i don ' t know . what about you ? [SEP] i had a bad headache . [SEP] what happened ? [SEP] i got it on the meeting . i can ’ t believe it must be a wonderful timeer . [SEP] well , you can ’ t go out for me . [SEP] what ' s that ? [SEP] i ' m sorry . i didn ' t know you were not busy at the beginning of vacation , and there are something interesting . [SEP] i\"]\n"
     ]
    }
   ],
   "source": [
    "## But it gives us some nice options such as easy completion of dialogues\n",
    "\n",
    "dialogues = [\"Hi how are you? \", \"what are you upto? [SEP]\"]\n",
    "\n",
    "print(lightning_language_model.complete_dialogues(dialogues, max_length=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4482b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
