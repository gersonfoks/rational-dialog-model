{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0084f62e",
   "metadata": {},
   "source": [
    "In this notebook we look at raitonalized language models. \n",
    "We first consider the one that looks at the models as a black box. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1061c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we fix the relative imports\n",
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fb43ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.LanguageModels.LstmLanguageModel import LSTMLanguageModel\n",
    "from modules.RationalExtractors.PolicyBasedRationalExtractor import PolicyBasedRationalExtractor\n",
    "from utils.utils import encode, decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2bce74d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from daily_dialog.DialogTokenizer import get_daily_dialog_tokenizer\n",
    "my_tokenizer = get_daily_dialog_tokenizer(tokenizer_location='../daily_dialog/tokenizer.json', )\n",
    "\n",
    "#Find the rmask token\n",
    "encode(my_tokenizer, '[RMASK]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1e562b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "location = \"../models/small_lm_test.pt\"\n",
    "trained_language_model = LSTMLanguageModel.load(location)\n",
    "rational_extractor = PolicyBasedRationalExtractor(my_tokenizer.get_vocab_size(), mask_token=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1485f030",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "context = \"Hi how are you today? [SEP]\"\n",
    "ids = encode(my_tokenizer, context).view(-1, 1) #Batch second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d412abbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'policy_logits': tensor([[[ 0.0587, -0.0585]],\n",
       " \n",
       "         [[ 0.1070,  0.0356]],\n",
       " \n",
       "         [[ 0.0642,  0.0869]],\n",
       " \n",
       "         [[-0.0495, -0.1706]],\n",
       " \n",
       "         [[ 0.1459, -0.2259]],\n",
       " \n",
       "         [[ 0.1012, -0.0150]],\n",
       " \n",
       "         [[ 0.0846, -0.0090]]], grad_fn=<AddBackward0>),\n",
       " 'policy': tensor([[[0.5293, 0.4707]],\n",
       " \n",
       "         [[0.5178, 0.4822]],\n",
       " \n",
       "         [[0.4943, 0.5057]],\n",
       " \n",
       "         [[0.5302, 0.4698]],\n",
       " \n",
       "         [[0.5919, 0.4081]],\n",
       " \n",
       "         [[0.5290, 0.4710]],\n",
       " \n",
       "         [[0.5234, 0.4766]]], grad_fn=<SoftmaxBackward>),\n",
       " 'chosen_policy': tensor([[[0.5293]],\n",
       " \n",
       "         [[0.5178]],\n",
       " \n",
       "         [[0.5057]],\n",
       " \n",
       "         [[0.5302]],\n",
       " \n",
       "         [[0.5919]],\n",
       " \n",
       "         [[0.4710]],\n",
       " \n",
       "         [[0.4766]]], grad_fn=<GatherBackward>),\n",
       " 'mask': tensor([[False],\n",
       "         [False],\n",
       "         [ True],\n",
       "         [False],\n",
       "         [False],\n",
       "         [ True],\n",
       "         [ True]]),\n",
       " 'masked_input': tensor([[427],\n",
       "         [219],\n",
       "         [  4],\n",
       "         [127],\n",
       "         [484],\n",
       "         [  4],\n",
       "         [  4]])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The policy based RE can mask the tokens directly:\n",
    "RE_out = rational_extractor.forward(ids)\n",
    "RE_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "832e21af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi how [RMASK] you today [RMASK] [RMASK]\n"
     ]
    }
   ],
   "source": [
    "# The masked input becomes:\n",
    "print(decode(my_tokenizer, RE_out[\"masked_input\"].flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da9be1f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 427,  219,    4,  127,  484,    4,    4,  157,   33,  127,  224,   73,\n",
      "          58,  171,  871,  296,  130,  159,  128, 1081,   16,  208,   47,  239,\n",
      "         127,   73,  206,  286, 1899,  234,   18,  127,  254,   16,  128,  296,\n",
      "          16,   47,  291, 1845,    5,  176, 2072,  159,  127,  209, 1707,   33,\n",
      "           1,  229,  699,   18,  152,   73,  302,  385,  724,  148,  128,  717,\n",
      "          18,  155,  152,   73,  206, 1051,  148,  157,  989,   18,    1,  252,\n",
      "          16,   47,  224,   11,   58,  254,  219,   47,   11,   51,   39,  402,\n",
      "         442,  148,  128, 1170,   18,  157,   11,   57,  990,   16,   47,  526,\n",
      "          11,   58,  288,  713])\n",
      "hi how [RMASK] you today [RMASK] [RMASK] that ? you don ’ t have enough time to do the movie , but i think you ’ re very careful about . you know , the time , i am kidding ! what classes do you like movies ? [SEP] not bad . we ’ ve been working in the office . and we ’ re interested in that country . [SEP] well , i don ' t know how i ' m a great day in the head . that ' s true , i didn ' t really care\n"
     ]
    }
   ],
   "source": [
    "# We then can forward the masked_input directly to the LM\n",
    "completed_dialogue = trained_language_model.complete_dialogue(RE_out[\"masked_input\"].flatten())\n",
    "print(completed_dialogue)\n",
    "print(decode(my_tokenizer, completed_dialogue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c7a728",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992cfadc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
