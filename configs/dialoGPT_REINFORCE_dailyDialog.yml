language_model:
  type: transformers
  load_location: './finetuning/saved_models/dialoGPT-daily_dialog-small'
  checkpoint: microsoft/DialoGPT-small
  pretrained: True
  save_location: './final_models/dialoGPT_dailyDialog_finetuned.pt'
dataset:
  type: 'daily_dialogue'
  size_train: 100
  size_test: 100
  batch_size: 5
  subsets: start
  remove_top_n: 0 #100
tokenizer:
  type: transformers
  checkpoint: 'microsoft/DialoGPT-small'
hparams:
  learning_rate: 0.001
  freeze_language_model: False
loss_module: crossentropyloss
trainer:
  type: policy #Type of the pytorch lighntning model we are training.
  max_epochs: 5
  log_every_n_steps: 1
rational_extractor:
  pretrained: False
  type: policy_based
  save_location: './models/RE_REINFORCE_dialoGPT.pt'
  load_location: './models/RE_REINFORCE_dialoGPT.pt'
  parameters:
    fussed_lasso_weight: 0.01
    sparsity_weight: 0.1