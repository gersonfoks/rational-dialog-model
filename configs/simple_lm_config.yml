language_model:
  type: LSTM
  pretrained: False
  embedding_dim: 128
  num_layers: 2
  hidden_state_size: 128
  save_location: './small_lm_test.pt'
dataset:
  type: 'daily_dialogue'
  size_train: 1000
  size_test: 10
  batch_size: 32
  subsets: start
tokenizer:
  type: 'daily_dialogue'
  link: './daily_dialog/tokenizer.json'
hparams:
  learning_rate: 0.001
loss_module: crossentropyloss
trainer:
  max_epochs: 50
  log_every_n_steps: 1



